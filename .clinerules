# GenreBender - AI-Powered Trailer Generator

## Project Overview
GenreBender is a sophisticated system for automatically generating cinematic trailers from full-length movies with genre transformation (e.g., turn a drama into a thriller trailer). It uses a complete 10-stage modular pipeline combining computer vision, audio analysis, multimodal AI (Qwen2-VL), GPT-4 for narrative generation, and FFmpeg for video assembly and audio mixing.

## Technology Stack
- **Python**: 3.9+ (primary language)
- **Video Processing**: FFmpeg, PySceneDetect, OpenCV
- **Audio Analysis**: librosa, soundfile
- **AI/ML**: Qwen2-VL (multimodal analysis with 4x GPU support), Azure OpenAI GPT-4 (narrative)
- **Configuration**: YAML files, python-dotenv
- **Async Processing**: Batch processing with checkpoints

## Project Structure
```
GenreBender/
├── main.py                           # Main pipeline orchestrator & CLI
├── run.sh                            # Convenience script for running pipeline
├── 1_shot_detection.py               # Stage 1 standalone script
├── 2_keyframe_extraction.py          # Stage 2 standalone script
├── 3_audio_extraction.py             # Stage 3 standalone script
├── 4_subtitle_management.py          # Stage 4 standalone script
├── 5_remote_analysis.py              # Stage 5 standalone script
├── 6_genre_scoring.py                # Stage 6 standalone script
├── 7_shot_selection.py               # Stage 7 standalone script
├── 8_narrative_generation.py         # Stage 8 standalone script
├── 9_video_assembly.py               # Stage 9 standalone script
├── 10_audio_mixing.py                # Stage 10 standalone script
├── pipeline_common.py                # Shared utilities for standalone scripts
├── requirements.txt                  # Python dependencies
├── .env                              # API keys (not in repo)
├── README.md                         # Main project documentation
├── PIPELINE_STAGES.md                # Stage-by-stage execution guide
├── qwen_server/                      # Qwen2-VL server implementation
│   ├── server.py                     # FastAPI server for multimodal analysis
│   ├── analyzer.py                   # Core analysis logic with multi-frame processing
│   ├── model_loader.py               # Model loading utilities
│   ├── config.yaml                   # Server configuration (model, API, GPU settings)
│   ├── prompts.yaml                  # Analysis prompts for different attributes
│   ├── requirements.txt              # Server-specific dependencies
│   ├── setup.sh                      # First-time setup (model download, venv)
│   ├── start_server.sh               # Start server (supports multiple instances)
│   ├── stop_server.sh                # Stop all running server instances
│   ├── README.md                     # Comprehensive server documentation
│   ├── .server_pids                  # Tracks running server PIDs
│   ├── server_8000.log               # Server logs (per port)
│   └── venv_qwen_server/             # Isolated virtual environment
├── trailer_generator/                # Main package
│   ├── checkpoint.py                 # Checkpoint/resume system
│   ├── config/ 
│   │   ├── settings.yaml             # Global configuration
│   │   └── genre_profiles.yaml       # Genre-specific scoring weights
│   ├── ingest/ 
│   │   ├── shot_detector.py          # Scene boundary detection
│   │   ├── keyframe_extractor.py     # Multi-frame extraction (5 per shot)
│   │   ├── audio_extractor.py        # MFCC & spectral features
│   │   └── batch_processor.py        # Batch processing utilities
│   ├── analysis/ 
│   │   ├── remote_analyzer.py        # Qwen2-VL API client with auth
│   │   ├── analysis_cache.py         # Analysis result caching
│   │   ├── genre_scorer.py           # Genre-based shot scoring
│   │   └── shot_selector.py          # Top shot selection
│   ├── narrative/ 
│   │   ├── azure_client.py           # Azure OpenAI client
│   │   ├── structure_prompts.py      # LLM prompt templates
│   │   └── timeline_generator.py     # Timeline creation logic
│   ├── assembly/                     # Stage 8 module
│   │   ├── video_assembler.py        # Video concatenation & color grading
│   │   ├── title_generator.py        # AI-powered title generation
│   │   └── transition_selector.py    # Transition selection logic
│   └── audio/                        # Stage 9 module
│       ├── audio_mixer.py            # Audio mixing engine
│       └── music_selector.py         # Music library & selection
├── audio_assets/                     # Audio resources
│   ├── music/                        # Music library (user-populated)
│   ├── effects/                      # Sound effects (future use)
│   ├── generated_cache/              # Cached generated audio
│   │   └── cache_index.json          # Audio generation cache index
│   └── README.md                     # Audio setup guide
├── outputs/                          # Generated output structure
│   └── <sanitized_filename>/ 
│       ├── shots/                    # Extracted video shots
│       ├── keyframes/                # Extracted frames (5 per shot)
│       ├── cache/                    # Analysis cache
│       ├── output/                   # Final outputs
│       │   ├── timeline.json         # Narrative timeline
│       │   ├── selected_shots.json   # Top-scored shots
│       │   ├── trailer_assembled.mp4 # Stage 8 output
│       │   └── trailer_final.mp4     # Stage 9 output (FINAL)
│       ├── temp/                     # Partial results & temp files
│       ├── checkpoint.json           # Pipeline state
│       └── trailer_generator.log     # Detailed logs
└── samples/                          # Sample videos for testing
    ├── caddyshack.mp4
    └── pixar_chess.mp4
```

## Development Guidelines

### Code Style
- Follow PEP 8 conventions
- Use type hints where appropriate
- Document complex functions with docstrings
- Keep functions focused and modular

### Configuration Management
- **settings.yaml**: Global processing parameters, API endpoints, batch sizes
- **genre_profiles.yaml**: Genre-specific scoring weights, color grading, pacing
- **qwen_server/config.yaml**: Qwen server configuration (model, GPU, API key)
- **qwen_server/prompts.yaml**: Analysis prompt templates for attributes
- **.env**: API keys (AZURE_OPENAI_KEY)
- Never commit API keys or credentials to repository

### Pipeline Stages (10-Stage Architecture)
1. **shot_detection**: Identify scene boundaries using PySceneDetect
2. **keyframe_extraction**: Extract 5 frames per shot for temporal analysis
3. **audio_extraction**: Extract MFCC, spectral centroid, RMS energy features
4. **subtitle_management**: Parse SRT files and map dialogue to shots
5. **remote_analysis**: Multimodal analysis via Qwen2-VL server
6. **genre_scoring**: Score shots based on genre profile weights
7. **shot_selection**: Select top N shots (default: 60)
8. **narrative_generation**: Generate coherent timeline with GPT-4 (enhanced with dialogue context)
9. **video_assembly**: Assemble video with color grading and transitions
10. **audio_mixing**: Mix music with audio ducking and normalization

## Common Workflows

### Running the Pipeline

**Unified Pipeline (main.py):**
```bash
# Basic usage
python main.py --input movie.mp4 --genre thriller

# Test mode (first 5 shots only)
python main.py --input clip.mp4 --genre action --test

# Resume from failed stage
python main.py --input movie.mp4 --genre horror --resume-from remote_analysis --skip-clean

# Force re-run specific stage (e.g., after config changes)
python main.py --input movie.mp4 --genre comedy --force-stage genre_scoring --skip-clean

# Reset and start fresh
python main.py --input movie.mp4 --genre thriller --reset-checkpoint

# Using convenience script
./run.sh movie.mp4 thriller
```

**Stage-by-Stage Execution:**
```bash
# Run stages individually for more control
python 1_shot_detection.py --input movie.mp4 --genre thriller
python 2_keyframe_extraction.py --input movie.mp4 --genre thriller
python 3_audio_extraction.py --input movie.mp4 --genre thriller
python 4_subtitle_management.py --input movie.mp4 --genre thriller
python 5_remote_analysis.py --input movie.mp4 --genre thriller
python 6_genre_scoring.py --input movie.mp4 --genre thriller
python 7_shot_selection.py --input movie.mp4 --genre thriller
python 8_narrative_generation.py --input movie.mp4 --genre thriller
python 9_video_assembly.py --input movie.mp4 --genre thriller
python 10_audio_mixing.py --input movie.mp4 --genre thriller

# Use --force to re-run a specific stage
python 6_genre_scoring.py --input movie.mp4 --genre horror --force

# Use --test for quick validation
python 1_shot_detection.py --input clip.mp4 --genre action --test
```

### Available Genres
- `thriller`: Suspenseful, building tension
- `action`: Fast-paced, high energy  
- `drama`: Emotional, character-driven
- `horror`: Atmospheric, frightening
- `scifi`: Futuristic, wonder-filled
- `comedy`: Upbeat, humorous
- `romance`: Warm, emotional connection

### Checkpoint System
- Automatically saves progress after each stage completion
- Validates input file and genre match before resuming
- Use `--resume-from STAGE` to skip completed stages
- Use `--force-stage STAGE` to re-run a specific stage (main.py)
- Use `--force` flag to re-run standalone stage scripts
- Use `--reset-checkpoint` to start completely fresh
- Always use `--skip-clean` when resuming to preserve existing work

### Output Structure
All outputs for a video are organized in `outputs/<sanitized_filename>/`:
- **shots/shot_metadata.json**: Complete shot information with analysis
- **output/timeline.json**: Narrative timeline structure
- **output/selected_shots.json**: Top-scored shots before timeline generation
- **output/trailer_assembled.mp4**: Stage 9 output (video without final audio)
- **output/trailer_final.mp4**: Stage 10 output (FINAL broadcast-ready trailer)
- **checkpoint.json**: Pipeline state for resuming
- **trailer_generator.log**: Detailed execution logs
- **cache/analysis_cache.json**: Cached multimodal analysis results

## External Dependencies

### Qwen2-VL Server
- **Multi-GPU Support**: Automatically uses all available GPUs (designed for 4x GPU setup)
- **Authentication**: Bearer token required (`Authorization: Bearer helloagorevski`)
- **Configuration**: `qwen_server/config.yaml` for model, server, and processing settings
- **Prompts**: Customizable analysis prompts in `qwen_server/prompts.yaml`
- **Lifecycle Management**: Use shell scripts for easy control

**Setup and Start:**
```bash
cd qwen_server
./setup.sh        # First-time setup (downloads ~4GB model)
./start_server.sh # Start server on port 8000
./stop_server.sh  # Stop all running instances
```

**Health Check:**
```bash
curl http://localhost:8000/health
```

**Expected Response:**
```json
{
  "status": "healthy",
  "model": "Qwen/Qwen2-VL-2B-Instruct",
  "device": "cuda",
  "gpu_count": 4,
  "gpu_memory_total": "80.0 GB"
}
```

**API Endpoints:**
- Health: `GET /health` (no auth required)
- Single analysis: `POST /analyze` (requires auth)
- Batch analysis: `POST /analyze_batch` (requires auth)

**Request Format:**
```json
{
  "shots": [{
    "shot_id": 1,
    "images": ["base64_img1", "base64_img2", ...],  // 5 frames
    "audio_features": {
      "mfcc_mean": [...],
      "spectral_centroid_mean": 2500.5,
      "rms_energy_mean": 0.045,
      ...
    },
    "start_time": 10.5,
    "end_time": 12.8,
    "duration": 2.3
  }]
}
```

**Response Format:**
```json
{
  "results": [{
    "shot_id": 1,
    "caption": "A person walking in a dark hallway",
    "attributes": {
      "suspense": 0.78,
      "darkness": 0.65,
      "emotional_tension": 0.59,
      "intensity": 0.82,
      "motion": 0.45,
      ...
    }
  }]
}
```

**Configuration in settings.yaml:**
```yaml
remote_analysis:
  qwen_server_url: "http://localhost:8000"  # Server URL
  batch_size: 10                             # Shots per batch
  retry_attempts: 3                          # Retry on failure
  timeout: 300                               # Request timeout (seconds)
```

### Azure OpenAI
- Requires valid endpoint and API key in `settings.yaml`
- Uses GPT-4 deployment for narrative generation
- API key can be in `.env` file as `AZURE_OPENAI_KEY`
- Temperature: 0.7 (configurable)
- Max tokens: 4000 (configurable)

### FFmpeg
- Must be installed and available in system PATH
- Used for video/audio extraction and processing
- Required for shot detection and keyframe extraction

## Important Constraints

### When Modifying Code:
1. **Preserve checkpoint compatibility**: Changes to stage names or data structures may break resume functionality
2. **Maintain shot metadata schema**: Other stages depend on consistent structure
3. **Respect batch processing**: Large videos require batching to avoid memory issues
4. **Cache invalidation**: Analysis cache keys include input path + shot times
5. **Error handling**: Pipeline should save partial results before failing
6. **Qwen server authentication**: All analysis requests require Bearer token
7. **Multi-GPU awareness**: Qwen server automatically distributes across GPUs

### Performance Considerations:
- Default batch size: 10 shots (configurable in `settings.yaml`)
- Keyframes: 5 frames per shot (temporal analysis requires multiple frames)
- Analysis caching significantly speeds up re-runs
- Test mode (`--test`) limits to first 5 shots for quick validation
- Qwen server with 4x GPUs: ~1-2 seconds per shot, ~8-15 seconds per batch (10 shots)
- Typical movie (500-1000 shots): ~8-20 minutes for analysis stage

### File Naming:
- Input filenames are sanitized for directory names (remove spaces, special chars)
- Output structure is consistent: `outputs/<sanitized_name>/`
- Shot files: `shot_0001.mp4`, `shot_0002.mp4`, etc.
- Keyframe files: `kf_0001_1.jpg` through `kf_0001_5.jpg` (5 frames per shot)

## Testing

### Test Mode
- Use `--test` flag to process only first 5 shots
- Useful for validating pipeline changes quickly
- Sample videos available in `samples/` directory (caddyshack.mp4, pixar_chess.mp4)

### Manual Testing Workflow
1. Test with `--test` flag first
2. Check logs in `outputs/<filename>/trailer_generator.log`
3. Verify checkpoint saves correctly
4. Test resume functionality with `--resume-from`
5. Validate final timeline.json structure
6. Test stage-by-stage execution for granular control

## Debugging

### Common Issues:
1. **"Qwen2-VL server not responding"**: 
   - Check server URL in settings.yaml
   - Verify server is running: `curl http://localhost:8000/health`
   - Check authentication token is correct
   - Review server logs: `cat qwen_server/server_8000.log`
2. **"FFmpeg not found"**: Ensure FFmpeg is in system PATH
3. **"Resume validation failed"**: Input file or genre mismatch, use `--reset-checkpoint`
4. **Out of memory**: 
   - Reduce `batch_size` in settings.yaml
   - Check Qwen server GPU memory: `nvidia-smi`
5. **librosa import error**: Install system dependencies (libsndfile1)
6. **"401 Unauthorized"**: Check Qwen server authentication token

### Log Locations:
- Main log: `outputs/<filename>/trailer_generator.log`
- Checkpoint state: `outputs/<filename>/checkpoint.json`
- Partial results: `outputs/<filename>/temp/partial_analysis.json`
- Qwen server logs: `qwen_server/server_8000.log`

### Useful Debug Commands:
```bash
# Check pipeline state
cat outputs/<filename>/checkpoint.json | jq

# View last 50 log lines
tail -n 50 outputs/<filename>/trailer_generator.log

# Check shot count
jq 'length' outputs/<filename>/shots/shot_metadata.json

# View timeline
jq . outputs/<filename>/output/timeline.json

# Check Qwen server status
curl http://localhost:8000/health

# Monitor GPU usage
nvidia-smi -l 1

# View Qwen server logs
tail -f qwen_server/server_8000.log

# Check running server PIDs
cat qwen_server/.server_pids
```

## Best Practices for AI Assistants

1. **Always check checkpoint state** before suggesting changes that affect pipeline stages
2. **Preserve backward compatibility** when modifying data structures
3. **Use `--test` mode** when testing code changes
4. **Respect the modular architecture** - each module has clear responsibilities
5. **Consider cache implications** when modifying analysis logic
6. **Update genre_profiles.yaml** when adding new scoring attributes
7. **Maintain consistent error handling** - save partial results, log errors
8. **Document API changes** if modifying server interfaces
9. **Test resume functionality** after pipeline modifications
10. **Keep output structure consistent** - external tools may depend on it
11. **Verify Qwen server is running** before executing stage 5 (remote_analysis)
12. **Use standalone scripts** for debugging specific stages
13. **Check authentication tokens** when modifying Qwen server interactions
14. **Monitor GPU memory** when processing large batches

## Music Library Setup

Place music files in `audio_assets/music/` with genre keywords:
- `thriller_suspense_01.mp3`
- `action_epic_music.wav`
- `horror_atmospheric.mp3`

Generated audio is cached in `audio_assets/generated_cache/` to avoid regeneration.

See `audio_assets/README.md` for detailed setup.

## AI Features (Optional)

Configure in `settings.yaml`:
```yaml
video:
  ai_title_generation: false      # GPT-4 title cards
  ai_transition_selection: false  # GPT-4 transition selection

audio:
  ai_music_selection: false       # GPT-4 music recommendation
```

**Cost:** ~$0.05-0.08 per trailer when all AI features enabled

## Documentation Files

- **README.md**: Main project overview and quick start
- **PIPELINE_STAGES.md**: Comprehensive guide for stage-by-stage execution
- **qwen_server/README.md**: Detailed Qwen server setup, API, and troubleshooting
- **audio_assets/README.md**: Music library setup and usage
- **.clinerules**: This file - development guidelines for AI assistants

## Shell Scripts

### Main Pipeline
- **run.sh**: Convenience wrapper for running main.py with common options

### Qwen Server Management
- **qwen_server/setup.sh**: First-time setup (venv, dependencies, model download)
- **qwen_server/start_server.sh**: Start Qwen server (supports multiple instances)
- **qwen_server/stop_server.sh**: Stop all running Qwen server instances

Usage:
```bash
# Setup (first time only)
cd qwen_server && ./setup.sh

# Start server
./start_server.sh

# Stop server
./stop_server.sh
```

## Shared Utilities

**pipeline_common.py**: Shared utilities for standalone stage scripts
- Configuration loading
- Output directory management
- Checkpoint handling
- Common argument parsing
- Logging setup
- File path utilities

Used by all standalone stage scripts (1_*.py through 10_*.py) to ensure consistency.

## Future Enhancement Notes
- Title card overlay rendering (drawtext filter)
- Advanced sound effects layer
- Beat detection for music sync
- Web UI dashboard
- Real-time preview mode
- Qwen server clustering for horizontal scaling
- Support for additional multimodal models
