# GenreBender - AI-Powered Trailer Generator

## Project Overview
GenreBender is a sophisticated system for automatically generating cinematic trailers from full-length movies with genre transformation (e.g., turn a drama into a thriller trailer). It uses a complete 10-stage modular pipeline combining computer vision, audio analysis, multimodal AI (Qwen2-VL), GPT-4 for narrative generation, and FFmpeg for video assembly and audio mixing.

## Technology Stack
- **Python**: 3.9+ (primary language)
- **Video Processing**: FFmpeg, PySceneDetect, OpenCV
- **Audio Analysis**: librosa, soundfile
- **AI/ML**: Qwen2-VL (multimodal analysis with 4x GPU support), Azure OpenAI GPT-4 (narrative)
- **Configuration**: YAML files, python-dotenv
- **Async Processing**: Batch processing with checkpoints

## Project Structure
```
GenreBender/
├── main.py                           # Main pipeline orchestrator & CLI
├── run_semantic_pipeline.sh          # Semantic pipeline orchestrator script
├── 1_shot_detection.py               # Stage 1 standalone script
├── 2_keyframe_extraction.py          # Stage 2 standalone script
├── 3_audio_extraction.py             # Stage 3 standalone script
├── 4_subtitle_management.py          # Stage 4 standalone script
├── 5_remote_analysis.py              # Stage 5 standalone script
├── 6_genre_scoring.py                # Stage 6 standalone script
├── 7_shot_selection.py               # Stage 7 standalone script
├── 8_narrative_generation.py         # Stage 8 standalone script
├── 9_video_assembly.py               # Stage 9 standalone script
├── 10_audio_mixing.py                # Stage 10 standalone script
├── pipeline_common.py                # Shared utilities for standalone scripts
├── requirements.txt                  # Python dependencies
├── .env                              # API keys (not in repo)
├── README.md                         # Main project documentation
├── qwen_server/                      # Qwen2-VL server implementation
│   ├── server.py                     # FastAPI server for multimodal analysis
│   ├── analyzer.py                   # Core analysis logic with multi-frame processing
│   ├── model_loader.py               # Model loading utilities
│   ├── config.yaml                   # Server configuration (model, API, GPU settings)
│   ├── prompts.yaml                  # Analysis prompts for different attributes
│   ├── requirements.txt              # Server-specific dependencies
│   ├── setup.sh                      # First-time setup (model download, venv)
│   ├── start_server.sh               # Start server (supports multiple instances)
│   ├── stop_server.sh                # Stop all running server instances
│   ├── README.md                     # Comprehensive server documentation
│   ├── .server_pids                  # Tracks running server PIDs
│   ├── server_8000.log               # Server logs (per port)
│   └── venv_qwen_server/             # Isolated virtual environment
├── trailer_generator/                # Main package
│   ├── checkpoint.py                 # Checkpoint/resume system
│   ├── config/ 
│   │   ├── settings.yaml             # Global configuration
│   │   └── genre_profiles.yaml       # Genre-specific scoring weights
│   ├── ingest/ 
│   │   ├── shot_detector.py          # Scene boundary detection
│   │   ├── keyframe_extractor.py     # Multi-frame extraction (5 per shot)
│   │   ├── audio_extractor.py        # MFCC & spectral features
│   │   └── batch_processor.py        # Batch processing utilities
│   ├── analysis/ 
│   │   ├── remote_analyzer.py        # Qwen2-VL API client with auth
│   │   ├── analysis_cache.py         # Analysis result caching
│   │   ├── genre_scorer.py           # Genre-based shot scoring
│   │   └── shot_selector.py          # Top shot selection
│   ├── narrative/ 
│   │   ├── azure_client.py           # Azure OpenAI client
│   │   ├── structure_prompts.py      # LLM prompt templates
│   │   └── timeline_generator.py     # Timeline creation logic
│   ├── assembly/                     # Stage 8 module
│   │   ├── video_assembler.py        # Video concatenation & color grading
│   │   ├── title_generator.py        # AI-powered title generation
│   │   └── transition_selector.py    # Transition selection logic
│   └── audio/                        # Stage 9 module
│       ├── audio_mixer.py            # Audio mixing engine
│       └── music_selector.py         # Music library & selection
├── audio_assets/                     # Audio resources
│   ├── music/                        # Music library (user-populated)
│   ├── effects/                      # Sound effects (future use)
│   ├── generated_cache/              # Cached generated audio
│   │   └── cache_index.json          # Audio generation cache index
│   └── README.md                     # Audio setup guide
├── outputs/                          # Generated output structure
│   └── <sanitized_filename>/ 
│       ├── shots/                    # Extracted video shots
│       ├── keyframes/                # Extracted frames (5 per shot)
│       ├── cache/                    # Analysis cache
│       ├── output/                   # Final outputs
│       │   ├── timeline.json         # Narrative timeline
│       │   ├── selected_shots.json   # Top-scored shots
│       │   ├── trailer_assembled.mp4 # Stage 8 output
│       │   └── trailer_final.mp4     # Stage 9 output (FINAL)
│       ├── temp/                     # Partial results & temp files
│       ├── checkpoint.json           # Pipeline state
│       └── trailer_generator.log     # Detailed logs
└── samples/                          # Sample videos for testing
    ├── caddyshack.mp4
    └── pixar_chess.mp4
```

## Development Guidelines

### Code Style
- Follow PEP 8 conventions
- Use type hints where appropriate
- Document complex functions with docstrings
- Keep functions focused and modular

### Configuration Management
- **settings.yaml**: Global processing parameters, API endpoints, batch sizes
- **genre_profiles.yaml**: Genre-specific scoring weights, color grading, pacing
- **qwen_server/config.yaml**: Qwen server configuration (model, GPU, API key)
- **qwen_server/prompts.yaml**: Analysis prompt templates for attributes
- **.env**: Environment variables for sensitive configuration (gitignored)
  - `AZURE_OPENAI_ENDPOINT`: Azure OpenAI endpoint URL (overrides settings.yaml)
  - `AZURE_OPENAI_KEY`: Azure OpenAI API key (overrides settings.yaml)
- Never commit API keys or credentials to repository
- Environment variables in `.env` take precedence over `settings.yaml` values

### Pipeline Stages (10-Stage Architecture)
1. **shot_detection**: Identify scene boundaries using PySceneDetect
2. **keyframe_extraction**: Extract 5 frames per shot for temporal analysis
3. **audio_extraction**: Extract MFCC, spectral centroid, RMS energy features
4. **subtitle_management**: Parse SRT files and map dialogue to shots
5. **remote_analysis**: Multimodal analysis via Qwen2-VL server
6. **genre_scoring**: Score shots based on genre profile weights
7. **shot_selection**: Select top N shots (default: 60)
8. **narrative_generation**: Generate coherent timeline with GPT-4 (enhanced with dialogue context)
9. **video_assembly**: Assemble video with color grading and transitions
10. **audio_mixing**: Mix music with audio ducking and normalization

## Common Workflows

### Running the Pipeline

**Unified Pipeline (main.py):**
```bash
# Basic usage
python main.py --input movie.mp4 --genre thriller

# Test mode (first 5 shots only)
python main.py --input clip.mp4 --genre action --test

# Resume from failed stage
python main.py --input movie.mp4 --genre horror --resume-from remote_analysis --skip-clean

# Force re-run specific stage (e.g., after config changes)
python main.py --input movie.mp4 --genre comedy --force-stage genre_scoring --skip-clean

# Reset and start fresh
python main.py --input movie.mp4 --genre thriller --reset-checkpoint

# Using semantic pipeline script (requires synopsis and subtitles)
./run_semantic_pipeline.sh movie.mp4 thriller "Movie Title" synopsis.txt movie.srt
```

**Stage-by-Stage Execution:**
```bash
# Run stages individually for more control
python 1_shot_detection.py --input movie.mp4 --genre thriller
python 2_keyframe_extraction.py --input movie.mp4 --genre thriller
python 3_audio_extraction.py --input movie.mp4 --genre thriller
python 4_subtitle_management.py --input movie.mp4 --genre thriller
python 5_remote_analysis.py --input movie.mp4 --genre thriller
python 6_genre_scoring.py --input movie.mp4 --genre thriller
python 7_shot_selection.py --input movie.mp4 --genre thriller
python 8_narrative_generation.py --input movie.mp4 --genre thriller
python 9_video_assembly.py --input movie.mp4 --genre thriller
python 10_audio_mixing.py --input movie.mp4 --genre thriller

# Use --force to re-run a specific stage
python 6_genre_scoring.py --input movie.mp4 --genre horror --force

# Use --test for quick validation
python 1_shot_detection.py --input clip.mp4 --genre action --test
```

### Available Genres
- `thriller`: Suspenseful, building tension
- `action`: Fast-paced, high energy  
- `drama`: Emotional, character-driven
- `horror`: Atmospheric, frightening
- `scifi`: Futuristic, wonder-filled
- `comedy`: Upbeat, humorous
- `romance`: Warm, emotional connection

### Checkpoint System
- Automatically saves progress after each stage completion
- Validates input file and genre match before resuming
- Use `--resume-from STAGE` to skip completed stages
- Use `--force-stage STAGE` to re-run a specific stage (main.py)
- Use `--force` flag to re-run standalone stage scripts
- Use `--reset-checkpoint` to start completely fresh
- Always use `--skip-clean` when resuming to preserve existing work

### Output Structure
All outputs for a video are organized in `outputs/<sanitized_filename>/`:
- **shots/shot_metadata.json**: Complete shot information with analysis
- **output/timeline.json**: Narrative timeline structure
- **output/selected_shots.json**: Top-scored shots before timeline generation
- **output/trailer_assembled.mp4**: Stage 9 output (video without final audio)
- **output/trailer_final.mp4**: Stage 10 output (FINAL broadcast-ready trailer)
- **checkpoint.json**: Pipeline state for resuming
- **trailer_generator.log**: Detailed execution logs
- **cache/analysis_cache.json**: Cached multimodal analysis results

## External Dependencies

### Qwen2-VL Server
- **Multi-GPU Support**: Automatically uses all available GPUs (designed for 4x GPU setup)
- **Authentication**: Bearer token required (`Authorization: Bearer helloagorevski`)
- **Configuration**: `qwen_server/config.yaml` for model, server, and processing settings
- **Prompts**: Customizable analysis prompts in `qwen_server/prompts.yaml`
- **Lifecycle Management**: Use shell scripts for easy control

**Setup and Start:**
```bash
cd qwen_server
./setup.sh        # First-time setup (downloads ~4GB model)
./start_server.sh # Start server on port 8000
./stop_server.sh  # Stop all running instances
```

**Health Check:**
```bash
curl http://localhost:8000/health
```

**Expected Response:**
```json
{
  "status": "healthy",
  "model": "Qwen/Qwen2-VL-2B-Instruct",
  "device": "cuda",
  "gpu_count": 4,
  "gpu_memory_total": "80.0 GB"
}
```

**API Endpoints:**
- Health: `GET /health` (no auth required)
- Single analysis: `POST /analyze` (requires auth)
- Batch analysis: `POST /analyze_batch` (requires auth)

**Request Format:**
```json
{
  "shots": [{
    "shot_id": 1,
    "images": ["base64_img1", "base64_img2", ...],  // 5 frames
    "audio_features": {
      "mfcc_mean": [...],
      "spectral_centroid_mean": 2500.5,
      "rms_energy_mean": 0.045,
      ...
    },
    "start_time": 10.5,
    "end_time": 12.8,
    "duration": 2.3
  }]
}
```

**Response Format:**
```json
{
  "results": [{
    "shot_id": 1,
    "caption": "A person walking in a dark hallway",
    "attributes": {
      "suspense": 0.78,
      "darkness": 0.65,
      "emotional_tension": 0.59,
      "intensity": 0.82,
      "motion": 0.45,
      ...
    }
  }]
}
```

**Configuration in settings.yaml:**
```yaml
remote_analysis:
  qwen_server_url: "http://localhost:8000"  # Server URL
  batch_size: 10                             # Shots per batch
  retry_attempts: 3                          # Retry on failure
  timeout: 300                               # Request timeout (seconds)
```

### Azure OpenAI
- Requires valid endpoint and API key in `settings.yaml`
- Uses GPT-4 deployment for narrative generation
- API key can be in `.env` file as `AZURE_OPENAI_KEY`
- Temperature: 0.7 (configurable)
- Max tokens: 4000 (configurable)

### FFmpeg
- Must be installed and available in system PATH
- Used for video/audio extraction and processing
- Required for shot detection and keyframe extraction

## Important Constraints

### When Modifying Code:
1. **Preserve checkpoint compatibility**: Changes to stage names or data structures may break resume functionality
2. **Maintain shot metadata schema**: Other stages depend on consistent structure
3. **Respect batch processing**: Large videos require batching to avoid memory issues
4. **Cache invalidation**: Analysis cache keys include input path + shot times
5. **Error handling**: Pipeline should save partial results before failing
6. **Qwen server authentication**: All analysis requests require Bearer token
7. **Multi-GPU awareness**: Qwen server automatically distributes across GPUs

### Performance Considerations:
- Default batch size: 10 shots (configurable in `settings.yaml`)
- Keyframes: 5 frames per shot (temporal analysis requires multiple frames)
- Analysis caching significantly speeds up re-runs
- Test mode (`--test`) limits to first 5 shots for quick validation
- Qwen server with 4x GPUs: ~1-2 seconds per shot, ~8-15 seconds per batch (10 shots)
- Typical movie (500-1000 shots): ~8-20 minutes for analysis stage

### File Naming:
- Input filenames are sanitized for directory names (remove spaces, special chars)
- Output structure is consistent: `outputs/<sanitized_name>/`
- Shot files: `shot_0001.mp4`, `shot_0002.mp4`, etc.
- Keyframe files: `kf_0001_1.jpg` through `kf_0001_5.jpg` (5 frames per shot)

## Testing

### Test Mode
- Use `--test` flag to process only first 5 shots
- Useful for validating pipeline changes quickly
- Sample videos available in `samples/` directory (caddyshack.mp4, pixar_chess.mp4)

### Manual Testing Workflow
1. Test with `--test` flag first
2. Check logs in `outputs/<filename>/trailer_generator.log`
3. Verify checkpoint saves correctly
4. Test resume functionality with `--resume-from`
5. Validate final timeline.json structure
6. Test stage-by-stage execution for granular control

## Debugging

### Common Issues:
1. **"Qwen2-VL server not responding"**: 
   - Check server URL in settings.yaml
   - Verify server is running: `curl http://localhost:8000/health`
   - Check authentication token is correct
   - Review server logs: `cat qwen_server/server_8000.log`
2. **"FFmpeg not found"**: Ensure FFmpeg is in system PATH
3. **"Resume validation failed"**: Input file or genre mismatch, use `--reset-checkpoint`
4. **Out of memory**: 
   - Reduce `batch_size` in settings.yaml
   - Check Qwen server GPU memory: `nvidia-smi`
5. **librosa import error**: Install system dependencies (libsndfile1)
6. **"401 Unauthorized"**: Check Qwen server authentication token

### Log Locations:
- Main log: `outputs/<filename>/trailer_generator.log`
- Checkpoint state: `outputs/<filename>/checkpoint.json`
- Partial results: `outputs/<filename>/temp/partial_analysis.json`
- Qwen server logs: `qwen_server/server_8000.log`

### Useful Debug Commands:
```bash
# Check pipeline state
cat outputs/<filename>/checkpoint.json | jq

# View last 50 log lines
tail -n 50 outputs/<filename>/trailer_generator.log

# Check shot count
jq 'length' outputs/<filename>/shots/shot_metadata.json

# View timeline
jq . outputs/<filename>/output/timeline.json

# Check Qwen server status
curl http://localhost:8000/health

# Monitor GPU usage
nvidia-smi -l 1

# View Qwen server logs
tail -f qwen_server/server_8000.log

# Check running server PIDs
cat qwen_server/.server_pids
```

## Best Practices for AI Assistants

1. **Always check checkpoint state** before suggesting changes that affect pipeline stages
2. **Preserve backward compatibility** when modifying data structures
3. **Use `--test` mode** when testing code changes
4. **Respect the modular architecture** - each module has clear responsibilities
5. **Consider cache implications** when modifying analysis logic
6. **Update genre_profiles.yaml** when adding new scoring attributes
7. **Maintain consistent error handling** - save partial results, log errors
8. **Document API changes** if modifying server interfaces
9. **Test resume functionality** after pipeline modifications
10. **Keep output structure consistent** - external tools may depend on it
11. **Verify Qwen server is running** before executing stage 5 (remote_analysis)
12. **Use standalone scripts** for debugging specific stages
13. **Check authentication tokens** when modifying Qwen server interactions
14. **Monitor GPU memory** when processing large batches

## Music Library Setup

Place music files in `audio_assets/music/` with genre keywords:
- `thriller_suspense_01.mp3`
- `action_epic_music.wav`
- `horror_atmospheric.mp3`

Generated audio is cached in `audio_assets/generated_cache/` to avoid regeneration.

See `audio_assets/README.md` for detailed setup.

## AI Features (Optional)

Configure in `settings.yaml`:
```yaml
video:
  ai_title_generation: false      # GPT-4 title cards
  ai_transition_selection: false  # GPT-4 transition selection

audio:
  ai_music_selection: false       # GPT-4 music recommendation
```

**Cost:** ~$0.05-0.08 per trailer when all AI features enabled

## Documentation Files

- **README.md**: Main project overview and quick start
- **qwen_server/README.md**: Detailed Qwen server setup, API, and troubleshooting
- **audio_assets/README.md**: Music library setup and usage
- **.clinerules**: This file - development guidelines for AI assistants

## Utilities

### Subtitle Parser (`utilities/subtitle_parser.py`)
Pure subtitle parsing utility for extracting dialogue from SRT files. Can be used independently of the main pipeline.

**Features:**
- Parses SRT files with pysrt library
- Extracts chronological dialogue with timestamps
- Filters by minimum duration
- Provides statistics (word count, duration, etc.)
- Multiple output formats (plain text, timestamped, formatted)

**Usage:**
```python
from utilities.subtitle_parser import SubtitleParser

# Initialize and load
parser = SubtitleParser(min_dialogue_duration=0.3)
parser.load_srt('movie.srt')

# Get statistics
stats = parser.get_statistics()  # entries, words, duration

# Get all entries
entries = parser.get_all_entries()  # List of dicts with timestamps

# Get full transcript
transcript = parser.get_full_transcript(include_timestamps=True)

# Get formatted subtitles
formatted = parser.get_formatted_subtitles()  # List of formatted strings
```

**Used by:**
- `11_story_graph_generator.py` - Story graph generation
- `trailer_generator/ingest/subtitle_extractor.py` - Pipeline subtitle processing

## Story Graph Generator - Stage 11 (`11_story_graph_generator.py`)

Standalone utility for generating comprehensive semantic story graphs from movies using GPT-4. Creates machine-readable JSON with characters, plot structure, scene timeline, and emotional arcs.

**Required Inputs:**
- Movie name (string)
- Synopsis (text or file path)
- Subtitles (SRT file path)

**Output Structure:**
```
outputs/story_graphs/<sanitized_movie_name>/
├── story_graph.json          # Main output
├── input_synopsis.txt        # Saved synopsis
├── input_subtitles.srt       # Copy of SRT
├── metadata.json             # Generation metadata
└── story_graph_generator.log # Detailed logs
```

**Usage Examples:**
```bash
# Basic usage
python 11_story_graph_generator.py \
  --movie-name "Dumb and Dumber" \
  --synopsis "Two friends embark on a cross-country road trip..." \
  --srt-file test_files/movie.srt

# Synopsis from file
python 11_story_graph_generator.py \
  --movie-name "Movie Title" \
  --synopsis synopsis.txt \
  --srt-file movie.srt

# Force overwrite
python 11_story_graph_generator.py \
  --movie-name "Movie" \
  --synopsis "..." \
  --srt-file movie.srt \
  --force

# Validate inputs only
python 11_story_graph_generator.py \
  --movie-name "Movie" \
  --synopsis "..." \
  --srt-file movie.srt \
  --validate-only
```

**Features:**
- Generates structured JSON with characters, plot, scenes, emotions
- Automatic token management (truncates long subtitles: 30%/40%/30% strategy)
- JSON schema validation
- Caching strategy: overwrites on re-run
- Processing time: ~30-60 seconds per movie

**JSON Output Schema:**
```json
{
  "title": "Movie Title",
  "logline": "One-sentence summary",
  "characters": [{
    "name": "Character Name",
    "description": "...",
    "motivations": ["..."],
    "relationships": {"Other": "relationship"}
  }],
  "major_themes": ["theme1", "theme2"],
  "plot_structure": {
    "setup": "...",
    "inciting_incident": "...",
    "rising_action": "...",
    "climax": "...",
    "resolution": "..."
  },
  "scene_timeline": [{
    "scene_id": 1,
    "start_time": "00:05:30",
    "end_time": "00:07:15",
    "summary": "...",
    "key_events": ["..."],
    "characters_present": ["..."],
    "dominant_emotion": "tense",
    "genre_indicators": ["..."],
    "visual_inferences": ["indoor", "night"]
  }],
  "emotional_arc": [{
    "scene_id": 1,
    "emotion": "calm",
    "intensity": 0.3
  }]
}
```

**Dependencies:**
- Azure OpenAI API key (configured in `settings.yaml`)
- `pysrt` library for subtitle parsing
- Valid SRT file with subtitles
- Synopsis (minimum 50 characters)

**Core Modules:**
- `trailer_generator/analysis/story_graph_generator.py` - Core generation logic
- `utilities/subtitle_parser.py` - Subtitle parsing
- `trailer_generator/narrative/azure_client.py` - GPT-4 integration

## Shell Scripts

### Semantic Pipeline
- **run_semantic_pipeline.sh**: Full semantic pipeline orchestrator (Stages 1-5 → 11-12 → 13-15 → 9-10)
  - Requires: video, genre, movie name, synopsis file, subtitle file
  - Usage: `./run_semantic_pipeline.sh movie.mp4 thriller "Movie Title" synopsis.txt movie.srt`

### Qwen Server Management
- **qwen_server/setup.sh**: First-time setup (venv, dependencies, model download)
- **qwen_server/start_server.sh**: Start Qwen server (supports multiple instances)
- **qwen_server/stop_server.sh**: Stop all running Qwen server instances

**Qwen Server Usage:**
```bash
# Setup (first time only)
cd qwen_server && ./setup.sh

# Start server
./start_server.sh

# Stop server
./stop_server.sh
```

**Semantic Pipeline Usage:**
```bash
# Run complete semantic trailer generation
./run_semantic_pipeline.sh test_files/hitch.mp4 thriller "Hitch" \
  test_files/hitch_synopsis.txt test_files/hitch.srt
```

## Beat Sheet Generator - Stage 12 / Layer 2.2 (`12_beat_sheet_generator.py`)

Transforms story graphs into genre-specific trailer beat sheets through two-stage LLM processing. This is **Layer 2.2** of the genre transformation pipeline, producing structured beats for automated scene retrieval.

**Required Inputs:**
- Story graph from Stage 11 (`story_graph.json`)
- Target genre selection

**Output Structure:**
```
outputs/story_graphs/<movie_name>/
├── story_graph.json          # Stage 11 input
├── genre_rewrite.json        # Stage 1 output (intermediate)
├── beats.json                # Stage 2 output (final)
├── metadata_beats.json       # Generation metadata
└── beat_sheet_generator.log  # Detailed logs
```

**Usage Examples:**
```bash
# Basic usage
python 12_beat_sheet_generator.py \
  --movie-name "Airplane!" \
  --target-genre thriller

# Force regeneration
python 12_beat_sheet_generator.py \
  --movie-name "Airplane!" \
  --target-genre horror \
  --force

# Custom temperature
python 12_beat_sheet_generator.py \
  --movie-name "Movie" \
  --target-genre action \
  --temperature 0.8

# Validate only
python 12_beat_sheet_generator.py \
  --movie-name "Movie" \
  --target-genre drama \
  --validate-only
```

**Two-Stage Process:**

1. **Genre Reinterpretation (LLM Call 1)**
   - Reframes the original movie in the target genre
   - Transforms tone, conflict, and emotional arc
   - Preserves core plot and characters
   - Outputs: `genre_rewrite.json` with:
     - new_genre, logline, primary_conflict
     - antagonistic_forces, genre_motifs
     - tone_profile (pace, visual_tone, sound_profile)
     - emotional_arc_transformed (opening → build → climax → resolution)

2. **Beat Sheet Generation (LLM Call 2)**
   - Creates 8-12 trailer beats from reinterpretation
   - Each beat includes:
     - id, name, description
     - target_emotion
     - visual_requirements (3-5 items)
     - audio_cue
     - voiceover (optional)
     - **embedding_prompt** (dense text for Layer 2.3 retrieval)
   - Follows standard trailer structure: hook → setup → conflict → escalation → climax

**Configuration (`settings.yaml`):**
```yaml
beat_sheet:
  temperature: 0.7              # LLM creativity (0.0-1.0)
  min_beats: 8                  # Minimum trailer beats
  max_beats: 12                 # Maximum trailer beats
  target_duration: 90           # Target trailer length (seconds)
  embedding_model: "text-embedding-ada-002"  # For Layer 2.3
  embedding_batch_size: 10      # Batch size for embeddings
```

**Features:**
- Two-stage LLM processing with structured JSON output
- Automatic genre context loading from `genre_profiles.yaml`
- Validates 8-12 beats, 3-5 visual requirements per beat
- Saves intermediate `genre_rewrite.json` for debugging
- Rich embedding prompts (50+ words) for semantic scene matching
- Processing time: ~10-20 seconds per beat sheet

**Core Module:**
- `trailer_generator/narrative/beat_sheet_generator.py` - Two-stage generator
  - `BeatSheetGenerator` class
  - `generate_beat_sheet()` - Main entry point
  - `_stage1_genre_reinterpretation()` - LLM Call 1
  - `_stage2_beat_sheet_generation()` - LLM Call 2
  - `_validate_beat_sheet()` - Schema validation
  - `_generate_embeddings()` - Placeholder for Layer 2.3

**Dependencies:**
- Azure OpenAI API key (configured in `settings.yaml`)
- Story graph from Stage 11
- Python: `openai`, `pyyaml` (already in requirements.txt)

**Integration Flow:**
```
Stage 11: Story Graph → Stage 12: Beat Sheet → Layer 2.3: Scene Retrieval
                         ↓
                   genre_rewrite.json (intermediate)
                   beats.json (with embedding prompts)
                         ↓
                   [Future: Semantic scene matching with embeddings]
```

**Common Issues:**
- **"Story graph not found"**: Run Stage 11 first
- **"Unsupported genre"**: Check `genre_profiles.yaml` for valid genres
- **Empty JSON response**: Verify Azure OpenAI configuration and `max_completion_tokens`
- **Beat count validation error**: Adjust `min_beats`/`max_beats` in settings.yaml

## Shared Utilities

**pipeline_common.py**: Shared utilities for standalone stage scripts
- Configuration loading
- Output directory management
- Checkpoint handling
- Common argument parsing
- Logging setup
- File path utilities

Used by all standalone stage scripts (1_*.py through 11_*.py) and beat sheet generator (12_*.py) to ensure consistency.

## Future Enhancement Notes
- Title card overlay rendering (drawtext filter)
- Advanced sound effects layer
- Beat detection for music sync
- Web UI dashboard
- Real-time preview mode
- Qwen server clustering for horizontal scaling
- Support for additional multimodal models
