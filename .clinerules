# GenreBender - AI-Powered Trailer Generator

## Project Overview
GenreBender is a sophisticated system for automatically generating cinematic trailers from full-length movies with genre transformation (e.g., turn a drama into a thriller trailer). It uses a complete 15-stage modular pipeline combining computer vision, audio analysis, multimodal AI (Qwen2-VL), GPT-4 for narrative generation, semantic embeddings for scene retrieval, and FFmpeg for video assembly and audio mixing.

## Technology Stack
- **Python**: 3.9+ (primary language)
- **Video Processing**: FFmpeg, PySceneDetect, OpenCV
- **Audio Analysis**: librosa, soundfile
- **AI/ML**: Qwen2-VL (multimodal analysis with multi-GPU support), Azure OpenAI GPT-4 (narrative), text-embedding-ada-002 (embeddings)
- **Vector Search**: FAISS for semantic scene retrieval
- **Configuration**: YAML files, python-dotenv
- **Async Processing**: Batch processing with checkpoints, parallel genre processing

## Project Structure
```
GenreBender/
├── run_multi_genre_pipeline.py       # Multi-genre pipeline orchestrator
├── 1_shot_detection.py               # Stage 1 standalone script
├── 2_keyframe_extraction.py          # Stage 2 standalone script
├── 3_audio_extraction.py             # Stage 3 standalone script
├── 4_subtitle_management.py          # Stage 4 standalone script
├── 5_remote_analysis.py              # Stage 5 standalone script
├── 6_genre_scoring.py                # Stage 6 standalone script (legacy)
├── 7_shot_selection.py               # Stage 7 standalone script (legacy)
├── 8_narrative_generation.py         # Stage 8 standalone script (legacy)
├── 9_video_assembly.py               # Stage 9 standalone script
├── 10_audio_mixing.py                # Stage 10 standalone script
├── 11_story_graph_generator.py       # Stage 11 standalone script
├── 12_beat_sheet_generator.py        # Stage 12 standalone script
├── 13_embedding_generator.py         # Stage 13 standalone script
├── 14_scene_retrieval.py             # Stage 14 standalone script
├── 15_timeline_constructor.py        # Stage 15 standalone script
├── config.yaml                       # Pipeline configuration (movies, genres)
├── pipeline_common.py                # Shared utilities for standalone scripts
├── requirements.txt                  # Python dependencies
├── .env                              # API keys (not in repo)
├── README.md                         # Main project documentation
├── qwen_server/                      # Qwen2-VL server implementation
│   ├── server.py                     # FastAPI server for multimodal analysis
│   ├── analyzer.py                   # Core analysis logic with multi-frame processing
│   ├── model_loader.py               # Model loading utilities
│   ├── config.yaml                   # Server configuration (model, API, GPU settings)
│   ├── prompts.yaml                  # Analysis prompts for different attributes
│   ├── requirements.txt              # Server-specific dependencies
│   ├── setup.sh                      # First-time setup (model download, venv)
│   ├── start_server.sh               # Start server (supports multiple instances)
│   ├── stop_server.sh                # Stop all running server instances
│   ├── tmux_launch.sh                # Launch multiple servers via tmux
│   ├── README.md                     # Comprehensive server documentation
│   ├── .server_pids                  # Tracks running server PIDs
│   ├── server_8000.log               # Server logs (per port)
│   └── venv_qwen_server/             # Isolated virtual environment
├── trailer_generator/                # Main package
│   ├── __init__.py
│   ├── checkpoint.py                 # Checkpoint/resume system (v2.0 with per-genre tracking)
│   ├── config/ 
│   │   ├── settings.yaml             # Global configuration
│   │   └── genre_profiles.yaml       # Genre-specific scoring weights (27 genres)
│   ├── ingest/ 
│   │   ├── shot_detector.py          # Scene boundary detection
│   │   ├── keyframe_extractor.py     # Multi-frame extraction (5 per shot)
│   │   ├── audio_extractor.py        # MFCC & spectral features
│   │   ├── subtitle_extractor.py     # Subtitle to shot mapping
│   │   └── batch_processor.py        # Batch processing utilities
│   ├── analysis/ 
│   │   ├── remote_analyzer.py        # Qwen2-VL API client with auth
│   │   ├── analysis_cache.py         # Analysis result caching
│   │   ├── genre_scorer.py           # Genre-based shot scoring
│   │   ├── shot_selector.py          # Top shot selection
│   │   ├── story_graph_generator.py  # Story graph generation
│   │   └── subtitle_chunker.py       # Subtitle chunking utility
│   ├── narrative/ 
│   │   ├── azure_client.py           # Azure OpenAI client
│   │   ├── beat_sheet_generator.py   # Two-stage beat sheet generation
│   │   ├── structure_prompts.py      # LLM prompt templates
│   │   ├── timeline_generator.py     # Legacy timeline creation
│   │   └── timeline_constructor.py   # Deterministic timeline construction
│   ├── embeddings/                   # Stage 13 module
│   │   ├── __init__.py
│   │   └── embedding_generator.py    # Scene & beat embedding generation
│   ├── retrieval/                    # Stage 14 module
│   │   ├── __init__.py
│   │   └── scene_retriever.py        # FAISS-based scene retrieval
│   ├── assembly/                     # Stage 9 module
│   │   ├── video_assembler.py        # Video concatenation & color grading
│   │   ├── title_generator.py        # AI-powered title generation
│   │   └── transition_selector.py    # Transition selection logic
│   └── audio/                        # Stage 10 module
│       ├── audio_mixer.py            # Audio mixing engine
│       └── music_selector.py         # Music library & selection
├── utilities/                        # Standalone utility modules
│   ├── __init__.py
│   ├── subtitle_parser.py            # Pure subtitle parsing utility
│   ├── omdb_client.py                # OMDB API client
│   ├── omdb_cache.py                 # OMDB response caching
│   ├── omdb_models.py                # Structured data models
│   ├── example_omdb.py               # OMDB usage examples
│   └── README.md                     # OMDB utilities documentation
├── audio_assets/                     # Audio resources
│   ├── music/                        # Music library (user-populated)
│   ├── effects/                      # Sound effects (future use)
│   ├── generated_cache/              # Cached generated audio
│   │   └── cache_index.json          # Audio generation cache index
│   └── README.md                     # Audio setup guide
├── outputs/                          # Generated output structure
│   ├── <sanitized_filename>/         # Per-video outputs
│   │   ├── shots/                    # Extracted video shots
│   │   ├── keyframes/                # Extracted frames (5 per shot)
│   │   ├── cache/                    # Analysis cache
│   │   ├── embeddings/               # Scene embeddings
│   │   ├── output/                   # Selected scenes, timeline
│   │   ├── trailers/<genre>/         # Genre-specific trailers
│   │   ├── temp/                     # Partial results & temp files
│   │   ├── checkpoint.json           # Pipeline state
│   │   └── trailer_generator.log     # Detailed logs
│   └── story_graphs/<movie_name>/    # Story graph outputs
│       ├── story_graph.json          # Semantic story structure
│       ├── beats_<genre>.json        # Genre-specific beat sheets
│       └── genre_rewrite_<genre>.json# Genre reinterpretations
└── test_files/                       # Test videos and synopses
    ├── hitch.mp4
    ├── hitch.srt
    ├── hitch_synopsis.txt
    ├── rolemodels.mp4
    ├── rolemodels.srt
    ├── rolemodels_synopsis.txt
    ├── pulpfiction.srt
    └── pulpfiction_synopsis.txt
```

## Development Guidelines

### Code Style
- Follow PEP 8 conventions
- Use type hints where appropriate
- Document complex functions with docstrings
- Keep functions focused and modular

### Configuration Management
- **config.yaml**: Root-level pipeline configuration (movies, genres, parallel workers)
- **settings.yaml**: Global processing parameters, API endpoints, batch sizes
- **genre_profiles.yaml**: Genre-specific scoring weights, color grading, pacing (27 genres)
- **qwen_server/config.yaml**: Qwen server configuration (model, GPU, API key)
- **qwen_server/prompts.yaml**: Analysis prompt templates for attributes
- **.env**: Environment variables for sensitive configuration (gitignored)
  - `AZURE_OPENAI_ENDPOINT`: Azure OpenAI endpoint URL (overrides settings.yaml)
  - `AZURE_OPENAI_KEY`: Azure OpenAI API key (overrides settings.yaml)
- Never commit API keys or credentials to repository
- Environment variables in `.env` take precedence over `settings.yaml` values

### Pipeline Stages (Multi-Genre Architecture)

**Genre-Agnostic Stages (run once, shared across all genres):**
1. **shot_detection**: Identify scene boundaries using PySceneDetect
2. **keyframe_extraction**: Extract 5 frames per shot for temporal analysis
3. **audio_extraction**: Extract MFCC, spectral centroid, RMS energy features
4. **subtitle_management**: Parse SRT files and map dialogue to shots
5. **remote_analysis**: Multimodal analysis via Qwen2-VL server
11. **story_graph_generation**: Generate semantic story graph from synopsis + subtitles

**Genre-Dependent Stages (run per genre, parallelizable):**
12. **beat_sheet_generation**: Transform story graph into genre-specific trailer beats
13. **embedding_generation**: Generate embeddings for semantic scene retrieval
14. **scene_retrieval**: Match beats to scenes using FAISS + multi-factor scoring
15. **timeline_construction**: Build deterministic shot-level timeline
9. **video_assembly**: Assemble video with genre-specific color grading and transitions
10. **audio_mixing**: Mix music with audio ducking and normalization

**Legacy Stages (deprecated, for old pipeline compatibility):**
6. **genre_scoring**: Score shots based on genre profile weights
7. **shot_selection**: Select top N shots (default: 60)
8. **narrative_generation**: Generate coherent timeline with GPT-4

## Common Workflows

### Running the Pipeline

**Multi-Genre Pipeline (recommended):**
```bash
# Generate trailers for all genres defined in config.yaml
python run_multi_genre_pipeline.py hitch

# Specify parallel workers (default: 4)
python run_multi_genre_pipeline.py hitch --parallel-workers 4

# Run sequentially with full output streaming (for debugging)
python run_multi_genre_pipeline.py hitch --sequential

# Skip Phase 1 if genre-agnostic stages are already complete
python run_multi_genre_pipeline.py hitch --skip-phase1

# Override genres from config.yaml
python run_multi_genre_pipeline.py hitch --genres comedy,thriller,horror

# Force re-run all stages
python run_multi_genre_pipeline.py hitch --force
```

**Stage-by-Stage Execution (Genre-Agnostic):**
```bash
# Run genre-agnostic stages (no --genre required)
python 1_shot_detection.py --input movie.mp4
python 2_keyframe_extraction.py --input movie.mp4
python 3_audio_extraction.py --input movie.mp4
python 4_subtitle_management.py --input movie.mp4 --srt-file movie.srt
python 5_remote_analysis.py --input movie.mp4

# Story graph generation (requires synopsis and subtitles)
python 11_story_graph_generator.py --movie-name "Movie" --synopsis synopsis.txt --srt-file movie.srt

# Use --force to re-run a specific stage
python 5_remote_analysis.py --input movie.mp4 --force

# Use --test for quick validation (limited shots)
python 1_shot_detection.py --input movie.mp4 --test
```

**Stage-by-Stage Execution (Genre-Dependent):**
```bash
# Run genre-dependent stages (--genre required)
python 12_beat_sheet_generator.py --movie-name "Movie" --genre thriller
python 13_embedding_generator.py --input movie.mp4 --genre thriller --movie-name "Movie"
python 14_scene_retrieval.py --input movie.mp4 --genre thriller --movie-name "Movie"
python 15_timeline_constructor.py --input movie.mp4 --genre thriller
python 9_video_assembly.py --input movie.mp4 --genre thriller
python 10_audio_mixing.py --input movie.mp4 --genre thriller

# Run same stages for different genres (can run in parallel)
python 12_beat_sheet_generator.py --movie-name "Movie" --genre horror
python 12_beat_sheet_generator.py --movie-name "Movie" --genre comedy
```

### Available Genres (27 total)

**Original Genres (12):**
- `comedy`: Upbeat, humorous
- `horror`: Atmospheric, frightening
- `thriller`: Suspenseful, building tension
- `parody`: Over-the-top, comedic exaggeration
- `mockumentary`: Documentary-style, deadpan humor
- `crime`: Noir-inspired, gritty investigation
- `drama`: Emotional, character-driven
- `experimental`: Surrealist, unconventional
- `fantasy`: Magical, epic adventure
- `romance`: Warm, emotional connection
- `scifi`: Futuristic, wonder-filled
- `action`: Fast-paced, high energy

**Major Traditional Genres (8):**
- `western`: Rugged, expansive landscapes
- `war`: Intense, heroic sacrifice
- `musical`: Theatrical, showstopping performances
- `documentary`: Authentic, observational storytelling
- `sports`: Triumphant, underdog journeys
- `mystery`: Intriguing, clue-driven reveals
- `historical`: Grand, period-authentic epics
- `biographical`: Inspiring, personal life journeys

**Emerging/Modern Genres (7):**
- `superhero`: Heroic, powerful, epic triumphs
- `dystopian`: Bleak, rebellious, industrial
- `found_footage`: Raw, authentic, immediate terror
- `kaiju`: Massive scale, monster destruction
- `cyberpunk`: Neon-soaked, gritty futurism
- `mumblecore`: Intimate, naturalistic indie
- `kdrama`: Emotional, romantic melodrama

### Checkpoint System
- Automatically saves progress after each stage completion
- **Version 2.0**: Supports per-genre completion tracking for genre-dependent stages
- Genre-agnostic stages are tracked globally (run once)
- Genre-dependent stages are tracked per-genre (can run in parallel)
- Use `--force` flag to re-run a specific stage
- Use `--reset-checkpoint` to start completely fresh
- Checkpoint file includes:
  - Global stage completion for genre-agnostic stages
  - Per-genre completion tracking in `genre_stages` section
  - Input file validation

### Output Structure
All outputs for a video are organized in `outputs/<sanitized_filename>/`:

**Shared (Genre-Agnostic) Outputs:**
- **shots/shot_metadata.json**: Complete shot information with analysis
- **keyframes/**: Extracted frames (5 per shot)
- **cache/analysis_cache.json**: Cached multimodal analysis results
- **embeddings/scene_embeddings.pkl**: Scene vector embeddings
- **checkpoint.json**: Pipeline state for resuming (includes per-genre tracking)
- **trailer_generator.log**: Detailed execution logs

**Genre-Specific Outputs:**
- **trailers/<genre>/trailer_<genre>_assembled.mp4**: Video without final audio
- **trailers/<genre>/trailer_<genre>_final.mp4**: FINAL broadcast-ready trailer
- **output/selected_scenes.json**: Beat-matched scene candidates
- **output/trailer_timeline.json**: Deterministic shot-level timeline

**Story Graph Outputs (in outputs/story_graphs/<movie_name>/):**
- **story_graph.json**: Semantic story structure (shared across genres)
- **beats_{genre}.json**: Genre-specific beat sheet (e.g., beats_thriller.json, beats_comedy.json)
- **genre_rewrite_{genre}.json**: Genre reinterpretation (e.g., genre_rewrite_thriller.json)

## External Dependencies

### Qwen2-VL Server
- **Multi-GPU Support**: Automatically uses all available GPUs (designed for multi-GPU setup)
- **Multi-Server Support**: Can run multiple servers on different ports (8000, 8001, 8002, 8003)
- **Authentication**: Bearer token required (`Authorization: Bearer helloagorevski`)
- **Configuration**: `qwen_server/config.yaml` for model, server, and processing settings
- **Prompts**: Customizable analysis prompts in `qwen_server/prompts.yaml`
- **Lifecycle Management**: Use shell scripts for easy control

**Setup and Start:**
```bash
cd qwen_server
./setup.sh        # First-time setup (downloads ~4GB model)
./start_server.sh # Start server on port 8000
./stop_server.sh  # Stop all running instances
```

**Health Check:**
```bash
curl http://localhost:8000/health
```

**Expected Response:**
```json
{
  "status": "healthy",
  "model": "Qwen/Qwen2-VL-2B-Instruct",
  "device": "cuda",
  "gpu_count": 4,
  "gpu_memory_total": "80.0 GB"
}
```

**API Endpoints:**
- Health: `GET /health` (no auth required)
- Single analysis: `POST /analyze` (requires auth)
- Batch analysis: `POST /analyze_batch` (requires auth)

**Request Format:**
```json
{
  "shots": [{
    "shot_id": 1,
    "images": ["base64_img1", "base64_img2", ...],
    "audio_features": {
      "mfcc_mean": [...],
      "spectral_centroid_mean": 2500.5,
      "rms_energy_mean": 0.045,
      ...
    },
    "start_time": 10.5,
    "end_time": 12.8,
    "duration": 2.3
  }]
}
```

**Response Format:**
```json
{
  "results": [{
    "shot_id": 1,
    "caption": "A person walking in a dark hallway",
    "attributes": {
      "suspense": 0.78,
      "darkness": 0.65,
      "emotional_tension": 0.59,
      "intensity": 0.82,
      "motion": 0.45,
      ...
    }
  }]
}
```

**Configuration in settings.yaml:**
```yaml
remote_analysis:
  server_host: "localhost"
  server_base_port: 8000
  server_count: 4                            # Multi-server support
  api_key: "helloagorevski"
  batch_size: 16
  load_balancing: "round_robin"
  timeout: 60
  max_retries: 3
```

### Azure OpenAI
- Requires valid endpoint and API key in `settings.yaml` or `.env`
- Uses GPT-4 deployment for narrative generation
- Uses text-embedding-ada-002 for embeddings
- API key can be in `.env` file as `AZURE_OPENAI_KEY`
- Endpoint can be in `.env` file as `AZURE_OPENAI_ENDPOINT`
- Temperature: 0.7 (configurable)
- Max tokens: 50000 (configurable)

### FFmpeg
- Must be installed and available in system PATH
- Used for video/audio extraction and processing
- Required for shot detection and keyframe extraction

## Important Constraints

### When Modifying Code:
1. **Preserve checkpoint compatibility**: Changes to stage names or data structures may break resume functionality
2. **Maintain shot metadata schema**: Other stages depend on consistent structure
3. **Respect batch processing**: Large videos require batching to avoid memory issues
4. **Cache invalidation**: Analysis cache keys include input path + shot times
5. **Error handling**: Pipeline should save partial results before failing
6. **Qwen server authentication**: All analysis requests require Bearer token
7. **Multi-GPU awareness**: Qwen server automatically distributes across GPUs
8. **Embedding consistency**: Scene and beat embeddings must use the same model

### Performance Considerations:
- Default batch size: 16 shots (configurable in `settings.yaml`)
- Keyframes: 5 frames per shot (temporal analysis requires multiple frames)
- Analysis caching significantly speeds up re-runs
- Test mode (`--test`) limits to first 5 shots for quick validation
- Qwen server with 4x GPUs: ~1-2 seconds per shot, ~8-15 seconds per batch
- Typical movie (500-1000 shots): ~8-20 minutes for analysis stage
- Embedding generation: ~20 parallel API requests for speed
- Scene retrieval uses FAISS for sub-millisecond similarity search

### File Naming:
- Input filenames are sanitized for directory names (remove spaces, special chars)
- Output structure is consistent: `outputs/<sanitized_name>/`
- Shot files: `shot_0001.mp4`, `shot_0002.mp4`, etc.
- Keyframe files: `kf_0001_1.jpg` through `kf_0001_5.jpg` (5 frames per shot)
- Genre-specific files: `beats_<genre>.json`, `trailer_<genre>_final.mp4`

## Testing

### Test Mode
- Use `--test` flag to process only first 5 shots
- Useful for validating pipeline changes quickly
- Sample videos available in `test_files/` directory

### Manual Testing Workflow
1. Test with `--test` flag first
2. Check logs in `outputs/<filename>/trailer_generator.log`
3. Verify checkpoint saves correctly
4. Test resume functionality with `--force`
5. Validate final timeline.json structure
6. Test stage-by-stage execution for granular control

## Debugging

### Common Issues:
1. **"Qwen2-VL server not responding"**: 
   - Check server URL in settings.yaml
   - Verify server is running: `curl http://localhost:8000/health`
   - Check authentication token is correct
   - Review server logs: `cat qwen_server/server_8000.log`
2. **"FFmpeg not found"**: Ensure FFmpeg is in system PATH
3. **"Resume validation failed"**: Input file or genre mismatch, use `--reset-checkpoint`
4. **Out of memory**: 
   - Reduce `batch_size` in settings.yaml
   - Check Qwen server GPU memory: `nvidia-smi`
5. **librosa import error**: Install system dependencies (libsndfile1)
6. **"401 Unauthorized"**: Check Qwen server authentication token
7. **"Embeddings not found"**: Run stage 13 first
8. **"Beat sheet not found"**: Run stage 12 first with matching genre
9. **FAISS import error**: Install faiss-cpu or faiss-gpu

### Log Locations:
- Main log: `outputs/<filename>/trailer_generator.log`
- Checkpoint state: `outputs/<filename>/checkpoint.json`
- Partial results: `outputs/<filename>/temp/partial_analysis.json`
- Qwen server logs: `qwen_server/server_8000.log`

### Useful Debug Commands:
```bash
# Check pipeline state
cat outputs/<filename>/checkpoint.json | jq

# View last 50 log lines
tail -n 50 outputs/<filename>/trailer_generator.log

# Check shot count
jq 'length' outputs/<filename>/shots/shot_metadata.json

# View timeline
jq . outputs/<filename>/output/trailer_timeline.json

# Check embeddings
ls -la outputs/<filename>/embeddings/

# Check Qwen server status
curl http://localhost:8000/health

# Monitor GPU usage
nvidia-smi -l 1

# View Qwen server logs
tail -f qwen_server/server_8000.log

# Check running server PIDs
cat qwen_server/.server_pids
```

## Best Practices for AI Assistants

1. **Always check checkpoint state** before suggesting changes that affect pipeline stages
2. **Preserve backward compatibility** when modifying data structures
3. **Use `--test` mode** when testing code changes
4. **Respect the modular architecture** - each module has clear responsibilities
5. **Consider cache implications** when modifying analysis logic
6. **Update genre_profiles.yaml** when adding new scoring attributes
7. **Maintain consistent error handling** - save partial results, log errors
8. **Document API changes** if modifying server interfaces
9. **Test resume functionality** after pipeline modifications
10. **Keep output structure consistent** - external tools may depend on it
11. **Verify Qwen server is running** before executing stage 5 (remote_analysis)
12. **Use standalone scripts** for debugging specific stages
13. **Check authentication tokens** when modifying Qwen server interactions
14. **Monitor GPU memory** when processing large batches
15. **Ensure embedding model consistency** between scene and beat embeddings

## Music Library Setup

Place music files in `audio_assets/music/` with genre keywords:
- `thriller_suspense_01.mp3`
- `action_epic_music.wav`
- `horror_atmospheric.mp3`

Generated audio is cached in `audio_assets/generated_cache/` to avoid regeneration.

See `audio_assets/README.md` for detailed setup.

## AI Features (Optional)

Configure in `settings.yaml`:
```yaml
video:
  ai_title_generation: true       # GPT-4 title cards
  ai_transition_selection: true   # GPT-4 transition selection

audio:
  ai_music_selection: false       # GPT-4 music recommendation
```

**Cost:** ~$0.05-0.08 per trailer when all AI features enabled

## Documentation Files

- **README.md**: Main project overview and quick start
- **qwen_server/README.md**: Detailed Qwen server setup, API, and troubleshooting
- **audio_assets/README.md**: Music library setup and usage
- **utilities/README.md**: OMDB API utilities documentation
- **.clinerules**: This file - development guidelines for AI assistants

## Utilities

### Subtitle Parser (`utilities/subtitle_parser.py`)
Pure subtitle parsing utility for extracting dialogue from SRT files. Can be used independently of the main pipeline.

**Features:**
- Parses SRT files with pysrt library
- Extracts chronological dialogue with timestamps
- Filters by minimum duration
- Provides statistics (word count, duration, etc.)
- Multiple output formats (plain text, timestamped, formatted)

**Usage:**
```python
from utilities.subtitle_parser import SubtitleParser

# Initialize and load
parser = SubtitleParser(min_dialogue_duration=0.3)
parser.load_srt('movie.srt')

# Get statistics
stats = parser.get_statistics()  # entries, words, duration

# Get all entries
entries = parser.get_all_entries()  # List of dicts with timestamps

# Get full transcript
transcript = parser.get_full_transcript(include_timestamps=True)

# Get formatted subtitles
formatted = parser.get_formatted_subtitles()  # List of formatted strings
```

**Used by:**
- `11_story_graph_generator.py` - Story graph generation
- `trailer_generator/ingest/subtitle_extractor.py` - Pipeline subtitle processing

### OMDB Utilities (`utilities/omdb_*.py`)
OMDB API client for fetching movie metadata with caching support.

**Features:**
- Structured data models (`OMDBMovie`, `OMDBRating`)
- Smart caching (30-day TTL)
- Error handling with custom exceptions
- Retry logic for network failures
- Multiple search methods (title, IMDb ID, query)

**Usage:**
```python
from utilities.omdb_client import OMDBClient
from utilities.omdb_cache import OMDBCache

# Initialize client
client = OMDBClient()

# Fetch movie by title
movie = client.get_movie_by_title("Hitch")
print(f"Title: {movie.title}")
print(f"Genre: {movie.genre}")
print(f"IMDb Rating: {movie.imdb_rating}")

# Use cache for efficiency
cache = OMDBCache(output_dir="outputs/hitch")
movie = cache.get_or_fetch("Hitch", client.get_movie_by_title)
```

**Configuration in settings.yaml:**
```yaml
omdb:
  api_key: "83fd90a1"      # Free tier (1,000 requests/day)
  cache_enabled: true
  cache_ttl: 2592000       # 30 days
```

See `utilities/README.md` for complete documentation.

## Story Graph Generator - Stage 11 (`11_story_graph_generator.py`)

Standalone utility for generating comprehensive semantic story graphs from movies using GPT-4. Creates machine-readable JSON with characters, plot structure, scene timeline, and emotional arcs.

**Required Inputs:**
- Movie name (string)
- Synopsis (text or file path)
- Subtitles (SRT file path)

**Output Structure:**
```
outputs/story_graphs/<movie_name>/
├── story_graph.json              # Main output (shared)
├── beats_{genre}.json            # Per-genre beat sheets
├── genre_rewrite_{genre}.json    # Per-genre reinterpretations
├── input_synopsis.txt            # Saved synopsis
├── input_subtitles.srt           # Copy of SRT
├── metadata.json                 # Generation metadata
└── story_graph_generator.log     # Detailed logs
```

**Usage Examples:**
```bash
# Basic usage
python 11_story_graph_generator.py \
  --movie-name "Dumb and Dumber" \
  --synopsis "Two friends embark on a cross-country road trip..." \
  --srt-file test_files/movie.srt

# Synopsis from file
python 11_story_graph_generator.py \
  --movie-name "Movie Title" \
  --synopsis synopsis.txt \
  --srt-file movie.srt

# Force overwrite
python 11_story_graph_generator.py \
  --movie-name "Movie" \
  --synopsis "..." \
  --srt-file movie.srt \
  --force

# Validate inputs only
python 11_story_graph_generator.py \
  --movie-name "Movie" \
  --synopsis "..." \
  --srt-file movie.srt \
  --validate-only
```

**Features:**
- Generates structured JSON with characters, plot, scenes, emotions
- Automatic token management (truncates long subtitles: 30%/40%/30% strategy)
- JSON schema validation
- Caching strategy: overwrites on re-run
- Processing time: ~30-60 seconds per movie

**JSON Output Schema:**
```json
{
  "title": "Movie Title",
  "logline": "One-sentence summary",
  "characters": [{
    "name": "Character Name",
    "description": "...",
    "motivations": ["..."],
    "relationships": {"Other": "relationship"}
  }],
  "major_themes": ["theme1", "theme2"],
  "plot_structure": {
    "setup": "...",
    "inciting_incident": "...",
    "rising_action": "...",
    "climax": "...",
    "resolution": "..."
  },
  "scene_timeline": [{
    "scene_id": 1,
    "start_time": "00:05:30",
    "end_time": "00:07:15",
    "summary": "...",
    "key_events": ["..."],
    "characters_present": ["..."],
    "dominant_emotion": "tense",
    "genre_indicators": ["..."],
    "visual_inferences": ["indoor", "night"]
  }],
  "emotional_arc": [{
    "scene_id": 1,
    "emotion": "calm",
    "intensity": 0.3
  }]
}
```

**Dependencies:**
- Azure OpenAI API key (configured in `settings.yaml`)
- `pysrt` library for subtitle parsing
- Valid SRT file with subtitles
- Synopsis (minimum 50 characters)

**Core Modules:**
- `trailer_generator/analysis/story_graph_generator.py` - Core generation logic
- `utilities/subtitle_parser.py` - Subtitle parsing
- `trailer_generator/narrative/azure_client.py` - GPT-4 integration

## Beat Sheet Generator - Stage 12 (`12_beat_sheet_generator.py`)

Transforms story graphs into genre-specific trailer beat sheets through two-stage LLM processing.

**Required Inputs:**
- Story graph from Stage 11 (`story_graph.json`)
- Target genre selection

**Output Structure:**
```
outputs/story_graphs/<movie_name>/
├── story_graph.json                  # Stage 11 input (shared)
├── beats_{genre}.json                # Stage 2 output (per genre)
├── genre_rewrite_{genre}.json        # Stage 1 output (per genre)
├── metadata_beats.json               # Generation metadata
└── beat_sheet_generator.log          # Detailed logs
```

**Usage Examples:**
```bash
# Basic usage
python 12_beat_sheet_generator.py \
  --movie-name "Airplane!" \
  --target-genre thriller

# Force regeneration
python 12_beat_sheet_generator.py \
  --movie-name "Airplane!" \
  --target-genre horror \
  --force
```

**Two-Stage Process:**

1. **Genre Reinterpretation (LLM Call 1)**
   - Reframes the original movie in the target genre
   - Transforms tone, conflict, and emotional arc
   - Outputs: `genre_rewrite_{genre}.json`

2. **Beat Sheet Generation (LLM Call 2)**
   - Creates 8-12 trailer beats from reinterpretation
   - Each beat includes: id, name, description, target_emotion, visual_requirements, audio_cue, voiceover, embedding_prompt
   - Outputs: `beats_{genre}.json`

**Configuration (`settings.yaml`):**
```yaml
beat_sheet:
  temperature: 0.7
  min_beats: 8
  max_beats: 12
  target_duration: 90
```

## Embedding Generator - Stage 13 (`13_embedding_generator.py`)

Generates vector embeddings for semantic scene retrieval using Azure OpenAI text-embedding-ada-002.

**Required Inputs:**
- Shot metadata from stages 1-5
- Beat sheet from stage 12

**Output Files:**
- `embeddings/scene_embeddings.pkl` - Scene vector embeddings
- `embeddings/beat_embeddings.pkl` - Beat vector embeddings

**Usage:**
```bash
python 13_embedding_generator.py \
  --input movie.mp4 \
  --genre thriller \
  --movie-name "Movie"
```

**Configuration (`settings.yaml`):**
```yaml
embedding:
  model: "text-embedding-ada-002"
  batch_size: 20
  max_input_tokens: 50000
  parallel_requests: true
  parallel_workers: 20
```

**Core Module:**
- `trailer_generator/embeddings/embedding_generator.py` - Embedding generation logic

## Scene Retrieval - Stage 14 (`14_scene_retrieval.py`)

Performs semantic beat-to-scene matching using FAISS and multi-factor scoring.

**Required Inputs:**
- Scene and beat embeddings from stage 13
- Shot metadata
- Beat sheet

**Output Files:**
- `output/selected_scenes.json` - Beat-matched scene candidates

**Usage:**
```bash
python 14_scene_retrieval.py \
  --input movie.mp4 \
  --genre thriller \
  --movie-name "Movie" \
  --top-k 10
```

**Multi-Factor Scoring:**
```yaml
retrieval:
  top_k: 10
  scoring_weights:
    semantic_similarity: 0.50    # Embedding cosine similarity
    emotional_alignment: 0.25    # Emotion matching
    visual_match: 0.20           # Visual attribute matching
    original_genre_penalty: 0.05 # Small penalty for original genre
```

**Core Module:**
- `trailer_generator/retrieval/scene_retriever.py` - FAISS-based retrieval

## Timeline Constructor - Stage 15 (`15_timeline_constructor.py`)

Builds deterministic shot-level timeline from beat-matched scenes.

**Required Inputs:**
- Selected scenes from stage 14

**Output Files:**
- `output/trailer_timeline.json` - Complete shot-level timeline

**Usage:**
```bash
python 15_timeline_constructor.py \
  --input movie.mp4 \
  --genre thriller \
  --target-duration 90
```

**Output Structure:**
```json
{
  "total_shots": 45,
  "actual_duration": 88.5,
  "shots": [...],
  "metadata": {
    "pacing_profile": {
      "avg_shot_duration": 1.97,
      "shots_per_minute": 30.5
    }
  }
}
```

**Configuration (`settings.yaml`):**
```yaml
timeline:
  target_duration: 90
  pacing_rules: null  # null = use defaults
```

**Core Module:**
- `trailer_generator/narrative/timeline_constructor.py` - Timeline construction logic

## Scripts

### Multi-Genre Pipeline
- **run_multi_genre_pipeline.py**: Full multi-genre pipeline orchestrator
  - Phase 1: Genre-agnostic stages (1-5, 11) run sequentially
  - Phase 2: Genre-dependent stages (12-15, 9-10) run in parallel per genre
  - Supports sequential mode for debugging (`--sequential`)
  - Usage: `python run_multi_genre_pipeline.py hitch --parallel-workers 4`

### Qwen Server Management
- **qwen_server/setup.sh**: First-time setup (venv, dependencies, model download)
- **qwen_server/start_server.sh**: Start Qwen server (supports multiple instances)
- **qwen_server/stop_server.sh**: Stop all running Qwen server instances
- **qwen_server/tmux_launch.sh**: Launch multiple servers via tmux

## Shared Utilities

**pipeline_common.py**: Shared utilities for standalone stage scripts
- Configuration loading (`load_config`, `load_pipeline_config`, `get_movie_config`)
- Output directory management (`get_output_base_dir`, `get_genre_output_dir`, `get_story_graph_dir`)
- Checkpoint handling (`initialize_stage`, `initialize_genre_stage`)
- Common argument parsing (`add_common_arguments`, `add_genre_arguments`)
- Logging setup (`setup_logging`)
- File path utilities (`sanitize_filename`, `get_genre_filename`)
- Genre list (`ALL_GENRES` - 12 core genres)

Used by all standalone stage scripts (1_*.py through 15_*.py) to ensure consistency.

## Genre Profiles

**38 Consolidated Attributes** used for scoring:
- suspense, ambiguity, emotional_intensity, darkness, intensity, motion, impact
- fear, futuristic, technology, wonder, scale, humor, lightheartedness, timing
- intimacy, warmth, beauty, absurdity, authenticity, awkwardness, deadpan
- mystery, surrealism, visual_abstraction, unconventional, magic, adventure
- landscape, ruggedness, sacrifice, patriotism, rhythm, performance, triumph
- rebellion, decay, romantic_tension

Each genre profile includes:
- **scoring_weights**: Attribute importance (subset of 38 attributes)
- **color_grade**: FFmpeg filter string and description
- **music_tags**: Keywords for music selection
- **music_generation**: Instruments per section (intro, build, climax, outro)
- **pacing**: Shot timing preference
- **text_overlay_style**: Font, color, shadow, animation

## Future Enhancement Notes
- Title card overlay rendering (drawtext filter)
- Advanced sound effects layer
- Beat detection for music sync
- Web UI dashboard
- Real-time preview mode
- Qwen server clustering for horizontal scaling
- Support for additional multimodal models
- Additional embedding models (e.g., OpenAI text-embedding-3-large)
